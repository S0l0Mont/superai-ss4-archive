{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":67478,"databundleVersionId":7734014,"sourceType":"competition"}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport os\nimport glob \nimport shutil\n\n# Read class data\nannotation_df = pd.read_csv(\"/kaggle/input/ultra-wide-band-pose-prediction/annotations.csv\")\nannotation_df","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-17T09:10:36.106266Z","iopub.execute_input":"2024-02-17T09:10:36.107102Z","iopub.status.idle":"2024-02-17T09:10:36.533811Z","shell.execute_reply.started":"2024-02-17T09:10:36.107057Z","shell.execute_reply":"2024-02-17T09:10:36.532692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -rf /kaggle/working/*","metadata":{"execution":{"iopub.status.busy":"2024-02-17T09:10:36.535682Z","iopub.execute_input":"2024-02-17T09:10:36.536031Z","iopub.status.idle":"2024-02-17T09:10:37.611758Z","shell.execute_reply.started":"2024-02-17T09:10:36.536002Z","shell.execute_reply":"2024-02-17T09:10:37.610315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create directories and class folders\ntry:\n    dir_name = \"/kaggle/working/raw_data\"\n    \n    os.makedirs(dir_name)\nexcept:\n    pass\nfor i in range(7):\n    try:\n        os.makedirs(os.path.join(dir_name, str(i)))\n    except:\n        continue\n\n# Copy files into thier classes\nindir = \"/kaggle/input/ultra-wide-band-pose-prediction/train/train\"\noutdir = \"/kaggle/working/raw_data\"\nin_testdir = \"/kaggle/input/ultra-wide-band-pose-prediction/test/test\"\n\n\nfor file_ in os.listdir(indir):\n    for i in range(len(annotation_df)):\n        if file_.split(\".npy\")[0] == annotation_df['id'].iloc[i]:\n            file_old = os.path.join(indir, file_)\n            file_new = os.path.join(outdir, str(annotation_df['class'].iloc[i]))\n            shutil.copy(file_old,file_new)\n        \n# Check number of file in each directories\nfor i in range(7):\n    print(f\"Class {i} =\", len(glob.glob(os.path.join(dir_name, str(i), \"*\"))))\nprint(\"Sum =\", len(glob.glob(os.path.join(dir_name, \"*\", \"*\"))))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-17T09:10:37.613423Z","iopub.execute_input":"2024-02-17T09:10:37.613788Z","iopub.status.idle":"2024-02-17T09:11:04.624046Z","shell.execute_reply.started":"2024-02-17T09:10:37.61373Z","shell.execute_reply":"2024-02-17T09:11:04.622938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# File preparation","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport glob\n\nbaseURL = \"/kaggle/working/raw_data\"\nbase_testURL = \"/kaggle/input/ultra-wide-band-pose-prediction/test/test/*.npy\"\n# Get all files from directories\n# e.g. file_list[i][j] -> Choose npy number j th from class i\nfile_list = []\nfile_testlist = glob.glob(base_testURL)\n\nfor i in range(7):\n    files = glob.glob(baseURL+'/'+str(i)+'/*')\n    files.sort()\n    file_list.append(files)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-17T09:11:04.627311Z","iopub.execute_input":"2024-02-17T09:11:04.628099Z","iopub.status.idle":"2024-02-17T09:11:04.656704Z","shell.execute_reply.started":"2024-02-17T09:11:04.628062Z","shell.execute_reply":"2024-02-17T09:11:04.655887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualization","metadata":{}},{"cell_type":"code","source":"# Get sample of image for testing\ntest_img_path = file_list[0][0]\ntest_img = np.load(test_img_path)\ntest_img = np.transpose(test_img)\ntest_img.shape","metadata":{"execution":{"iopub.status.busy":"2024-02-17T09:11:04.657768Z","iopub.execute_input":"2024-02-17T09:11:04.658035Z","iopub.status.idle":"2024-02-17T09:11:04.669302Z","shell.execute_reply.started":"2024-02-17T09:11:04.65801Z","shell.execute_reply":"2024-02-17T09:11:04.66813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Distance","metadata":{}},{"cell_type":"markdown","source":"### Surface visualization","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport plotly.graph_objs as go\nimport cmath\n\n# Create a meshgrid of x and y values\nx = np.arange(0, test_img.shape[1])\ny = np.arange(0, test_img.shape[0])\nX, Y = np.meshgrid(x, y)\n\n# Set up amplitude to distance (absolute of data)\nZ = np.abs(test_img)\n\n# Create a surface plot of the data using plotly\nfig = go.Figure(data=[go.Surface(x=X, y=Y, z=Z)])\n\n# Set the axis titles\nfig.update_layout(scene=dict(xaxis_title='Time (1/256 sec)',\n                             yaxis_title='Sampler index',\n                             zaxis_title='Distance'))\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-17T09:11:04.670763Z","iopub.execute_input":"2024-02-17T09:11:04.671043Z","iopub.status.idle":"2024-02-17T09:11:05.251449Z","shell.execute_reply.started":"2024-02-17T09:11:04.671019Z","shell.execute_reply":"2024-02-17T09:11:05.249702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.imshow(np.abs(test_img), aspect='auto')","metadata":{"execution":{"iopub.status.busy":"2024-02-17T09:11:05.253184Z","iopub.execute_input":"2024-02-17T09:11:05.253513Z","iopub.status.idle":"2024-02-17T09:11:05.627592Z","shell.execute_reply.started":"2024-02-17T09:11:05.253483Z","shell.execute_reply":"2024-02-17T09:11:05.626689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Velocity","metadata":{}},{"cell_type":"markdown","source":"### Surface visualization","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport plotly.graph_objs as go\nimport cmath\n\n# Create a meshgrid of x and y values\nx = np.arange(0, test_img.shape[1])\ny = np.arange(0, test_img.shape[0])\nX, Y = np.meshgrid(x, y)\n\n# Set up amplitude to velocity (arctan of data)\nZ = np.angle(test_img)\n\n# Create a surface plot of the data using plotly\nfig = go.Figure(data=[go.Surface(x=X, y=Y, z=Z)])\n\n# Set the axis titles\nfig.update_layout(scene=dict(xaxis_title='Time (1/256 sec)',\n                             yaxis_title='Sampler index',\n                             zaxis_title='Velocity'))\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-17T09:11:05.628733Z","iopub.execute_input":"2024-02-17T09:11:05.629025Z","iopub.status.idle":"2024-02-17T09:11:05.70593Z","shell.execute_reply.started":"2024-02-17T09:11:05.628999Z","shell.execute_reply":"2024-02-17T09:11:05.7042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.imshow(np.angle(test_img), aspect='auto')","metadata":{"execution":{"iopub.status.busy":"2024-02-17T09:11:05.707708Z","iopub.execute_input":"2024-02-17T09:11:05.708105Z","iopub.status.idle":"2024-02-17T09:11:06.075354Z","shell.execute_reply.started":"2024-02-17T09:11:05.708065Z","shell.execute_reply":"2024-02-17T09:11:06.074383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set class name for displaying\nclass_name = [\n    'Stumble',\n    'Jump',\n    'Lay down',\n    'Run',\n    'Stand -> Sit',\n    'Sit -> Stand/Walk',\n    'Walk'\n]","metadata":{"execution":{"iopub.status.busy":"2024-02-17T09:11:06.079635Z","iopub.execute_input":"2024-02-17T09:11:06.079978Z","iopub.status.idle":"2024-02-17T09:11:06.085043Z","shell.execute_reply.started":"2024-02-17T09:11:06.07995Z","shell.execute_reply":"2024-02-17T09:11:06.084004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Method1: Using filter\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport scipy.signal as signal\n\n# Band-pass filter\ndef bp_filter_signal(complex_img, low=0.008, high=0.1):\n\n  # === Band-pass filter === #\n    img_fft = np.fft.fft2(complex_img)\n\n    low_cutoff = low\n    high_cutoff = high\n\n  # Create a mask for the filter function\n    ny, nx = img_fft.shape\n    u, v = np.meshgrid(np.fft.fftfreq(nx), np.fft.fftfreq(ny))\n    d = np.sqrt(u**2 + v**2)\n    mask = np.logical_and(d > low_cutoff, d < high_cutoff)\n\n  # Apply the filter to the Fourier transform\n    bp_img = img_fft * mask\n\n  # Compute the inverse Fourier transform to get the filtered image\n    filter_img = np.fft.ifft2(bp_img).real\n    return filter_img\n\n# High-pass then Band-pass filter\ndef hpbp_filter_signal(complex_img, high_cut=0.0005, low=0.008, high=0.1):\n  # === High-pass filter === #\n    filter_order = 3\n    b, a = signal.butter(filter_order, high_cut, 'high')\n    hp_img = signal.filtfilt(b, a, complex_img)\n\n  # === Band-pass filter === #\n    img_fft = np.fft.fft2(hp_img)\n    low_cutoff = low\n    high_cutoff = high\n\n  # Create a mask for the filter function\n    ny, nx = img_fft.shape\n    u, v = np.meshgrid(np.fft.fftfreq(nx), np.fft.fftfreq(ny))\n    d = np.sqrt(u**2 + v**2)\n    mask = np.logical_and(d > low_cutoff, d < high_cutoff)\n\n  # Apply the filter to the Fourier transform\n    bp_img = img_fft * mask\n\n  # Compute the inverse Fourier transform to get the filtered image\n    filter_img = np.fft.ifft2(bp_img).real\n    return filter_img\n\n# Band-pass then High-pass filter\ndef bphp_filter_signal(complex_img, high_cut=0.0005, low=0.008, high=0.1):\n  \n  # === Band-pass filter === #\n    img_fft = np.fft.fft2(complex_img)\n\n    low_cutoff = low\n    high_cutoff = high\n\n  # Create a mask for the filter function\n    ny, nx = img_fft.shape\n    u, v = np.meshgrid(np.fft.fftfreq(nx), np.fft.fftfreq(ny))\n    d = np.sqrt(u**2 + v**2)\n    mask = np.logical_and(d > low_cutoff, d < high_cutoff)\n\n  # Apply the filter to the Fourier transform\n    bp_img = img_fft * mask\n\n  # Compute the inverse Fourier transform to get the filtered image\n    bp_img = np.fft.ifft2(bp_img).real\n\n  # === High-pass filter === #\n    filter_order = 3\n    b, a = signal.butter(filter_order, high_cut, 'high')\n    filter_img = signal.filtfilt(b, a, bp_img)\n\n    return filter_img","metadata":{"execution":{"iopub.status.busy":"2024-02-17T09:11:06.086622Z","iopub.execute_input":"2024-02-17T09:11:06.08698Z","iopub.status.idle":"2024-02-17T09:11:06.650633Z","shell.execute_reply.started":"2024-02-17T09:11:06.086946Z","shell.execute_reply":"2024-02-17T09:11:06.649224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Result","metadata":{}},{"cell_type":"markdown","source":"Filter with Band-pass filter","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(5, 7, figsize=(20, 10))\n\n# Iterate over the images and plot each one\nfor i in range(5):\n    for j in range(7):\n        sample_img = np.load(file_list[j][i])\n        sample_img = np.transpose(sample_img)\n        distance_img = np.abs(sample_img)\n        filter_img = bp_filter_signal(distance_img, 0.008, 0.1)     # Using bp filter\n        filter_img = filter_img.clip(min=0)                       # set minimum value to 0\n        axs[i][j].imshow(filter_img, cmap='jet', aspect='auto')\n        if i==0:\n            axs[i][j].set_title(class_name[j])\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-17T09:11:06.65211Z","iopub.execute_input":"2024-02-17T09:11:06.652712Z","iopub.status.idle":"2024-02-17T09:11:12.738435Z","shell.execute_reply.started":"2024-02-17T09:11:06.652671Z","shell.execute_reply":"2024-02-17T09:11:12.73741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Filter with Band-pass then High-pass filter","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(5, 7, figsize=(20, 10))\n\n# Iterate over the images and plot each one\nfor i in range(5):\n    for j in range(7):\n        sample_img = np.load(file_list[j][i])\n        sample_img = np.transpose(sample_img)\n        distance_img = np.abs(sample_img)\n        filter_img = bphp_filter_signal(distance_img, 0.004, 0.0001, 1)\n        filter_img = filter_img.clip(min=0)\n        axs[i][j].imshow(filter_img, cmap='jet', aspect='auto')\n        if i==0:\n            axs[i][j].set_title(class_name[j])\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-17T09:11:12.739785Z","iopub.execute_input":"2024-02-17T09:11:12.740828Z","iopub.status.idle":"2024-02-17T09:11:18.881034Z","shell.execute_reply.started":"2024-02-17T09:11:12.740791Z","shell.execute_reply":"2024-02-17T09:11:18.880062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Method2: Using Wavelet transform","metadata":{}},{"cell_type":"markdown","source":"### Normal size","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pywt\nfrom PIL import Image\n\ndef wavelet_transform(complex_img):\n  # Perform the 2D Wavelet transform using the Haar wavelet\n    coeffs = pywt.dwt2(complex_img, 'haar')\n\n  # Get the approximation and detail coefficients\n    cA, (cH, cV, cD) = coeffs\n\n  # Calculate the magnitude of the complex numbers in the approximation coefficients\n    mag_wav = np.abs(cA)\n\n  # Normalize the magnitude to the range [0, 255]\n    wavelet_img = 255.0 * mag_wav / np.max(mag_wav)\n\n    return mag_wav\n","metadata":{"execution":{"iopub.status.busy":"2024-02-17T09:11:18.882429Z","iopub.execute_input":"2024-02-17T09:11:18.882708Z","iopub.status.idle":"2024-02-17T09:11:18.963546Z","shell.execute_reply.started":"2024-02-17T09:11:18.882683Z","shell.execute_reply":"2024-02-17T09:11:18.962704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(5, 7, figsize=(20, 10))\n\n# Iterate over the images and plot each one\nfor i in range(5):\n    for j in range(7):\n        sample_img = np.load(file_list[j][i])\n        distance_img = np.abs(sample_img)\n        filter_img = wavelet_transform(distance_img)\n        filter_img = np.transpose(filter_img)\n        filter_img = filter_img.clip(min=0)\n        axs[i][j].imshow(filter_img, cmap='jet', aspect='auto')\n    if i==0:\n        axs[i][j].set_title(class_name[j])\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-17T09:11:18.96472Z","iopub.execute_input":"2024-02-17T09:11:18.965206Z","iopub.status.idle":"2024-02-17T09:11:24.370214Z","shell.execute_reply.started":"2024-02-17T09:11:18.965178Z","shell.execute_reply":"2024-02-17T09:11:24.369246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Up-sampling","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pywt\nfrom PIL import Image\n\ndef wavelet_upsampling(complex_img, upsamp_factor=2):\n  # Perform the 2D Wavelet transform using the Haar wavelet\n    coeffs = pywt.dwt2(complex_img, 'haar')\n\n  # Get the approximation and detail coefficients\n    cA, (cH, cV, cD) = coeffs\n\n  # Calculate the magnitude of the complex numbers in the approximation coefficients\n    mag_wav = np.abs(cA)\n\n  # Normalize the magnitude to the range [0, 255]\n    wavelet_img = 255.0 * mag_wav / np.max(mag_wav)\n\n  # Upsample the image\n    wavelet_img = np.kron(wavelet_img, np.ones((upsamp_factor,upsamp_factor)))\n\n    return wavelet_img\n","metadata":{"execution":{"iopub.status.busy":"2024-02-17T09:11:24.371445Z","iopub.execute_input":"2024-02-17T09:11:24.371729Z","iopub.status.idle":"2024-02-17T09:11:24.378433Z","shell.execute_reply.started":"2024-02-17T09:11:24.371701Z","shell.execute_reply":"2024-02-17T09:11:24.377511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(5, 7, figsize=(20, 10))\n\n# Iterate over the images and plot each one\nfor i in range(5):\n    for j in range(7):\n        sample_img = np.load(file_list[j][i])\n        distance_img = np.abs(sample_img)\n        filter_img = wavelet_upsampling(distance_img)\n        filter_img = np.transpose(filter_img)\n        filter_img = filter_img.clip(min=0)\n        axs[i][j].imshow(filter_img, cmap='jet', aspect='auto')\n        if i==0:\n            axs[i][j].set_title(class_name[j])\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-17T09:11:24.379544Z","iopub.execute_input":"2024-02-17T09:11:24.37983Z","iopub.status.idle":"2024-02-17T09:11:30.305046Z","shell.execute_reply.started":"2024-02-17T09:11:24.379806Z","shell.execute_reply":"2024-02-17T09:11:30.304021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Method3","metadata":{}},{"cell_type":"code","source":"def range_time(IQ_data):\n    n_rd_history = 256\n    frame = []\n    frames = []\n\n    for iqini in IQ_data:\n        if len(frame)<n_rd_history:\n            frame.append(iqini)\n        else:  \n            frames.append(np.copy(frame))\n            frame.append(iqini)\n            frame = frame[1::]\n            \n    return np.stack(frames)\n\ndef range_frequency(datas):\n    Range_frequency_frame = []\n    for data in datas:\n        # Range-Doppler\n        rd = np.fft.fft(data, axis=0)\n        rd = np.fft.fftshift(rd, axes=0)\n        rd = np.abs(rd)\n        DBrd = 20 * np.log10(rd+1e-10)\n        Range_frequency_frame.append(DBrd)\n    return np.stack(Range_frequency_frame)\n\ndef srf_transform(complex_img, half=False):\n    img = range_time(complex_img)\n    img = range_frequency(img)\n    if half:\n        img = img[:, :img.shape[1]//2, :]\n    srf_img = img.reshape(img.shape[0]*img.shape[1], img.shape[2]).real\n    return srf_img.T","metadata":{"execution":{"iopub.status.busy":"2024-02-17T09:11:30.306411Z","iopub.execute_input":"2024-02-17T09:11:30.306756Z","iopub.status.idle":"2024-02-17T09:11:30.31748Z","shell.execute_reply.started":"2024-02-17T09:11:30.306708Z","shell.execute_reply":"2024-02-17T09:11:30.31645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(2, 7, figsize=(20, 4))\n\n# Iterate over the images and plot each one\nfor i in range(2):\n    for j in range(7):\n        sample_img = np.load(file_list[j][i])\n        distance_img = np.abs(sample_img)\n        filter_img = srf_transform(distance_img)\n        axs[i][j].imshow(filter_img, cmap='jet', aspect='auto')\n        if i==0:\n            axs[i][j].set_title(class_name[j])\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-17T09:11:30.318829Z","iopub.execute_input":"2024-02-17T09:11:30.319155Z","iopub.status.idle":"2024-02-17T09:12:26.163181Z","shell.execute_reply.started":"2024-02-17T09:11:30.319125Z","shell.execute_reply":"2024-02-17T09:12:26.162136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save image","metadata":{}},{"cell_type":"code","source":"from PIL import Image\nimport matplotlib.pyplot as plt\nimport matplotlib\nfrom tqdm import tqdm\nimport os\nmatplotlib.use('Agg')\noutput_dir = \"/kaggle/working/out_pre\"    # : EDIT HERE : Add your output directory where the full output path is <your path>/train/class_id/filename.png\ndpi = 100                     # : EDIT HERE : Select you dpi of output image\nheight = 224                  # : EDIT HERE : Select you image height\nwidth = 224                   # : EDIT HERE : Select you image width\n\nos.makedirs(output_dir, exist_ok=True)\nfor i in range(7):\n    os.makedirs(output_dir+\"/train/\"+str(i), exist_ok=True)\n# Export images\n\n\nfor i, class_files in enumerate(file_list):\n    for j, file_path in enumerate(tqdm(class_files)):\n    # select image\n        image = np.load(file_list[i][j])\n        distance_img = np.abs(image)                              # : EDIT HERE : select your amplitude (abs: distance, angle: velocity)\n        filter_img = srf_transform(distance_img)                  # : EDIT HERE : select your function\n\n    # SAVE FIG\n        fig, axs = plt.subplots(1, 1, figsize=(width/dpi, height/dpi))\n        axs.imshow(filter_img, cmap='jet', aspect='auto')         # : EDIT HERE : select your camp\n        axs.axis('off')\n        filename = file_path.split('/')[-1].replace('.npy', '.png')\n        fig.savefig(os.path.join(output_dir, \"train\", str(i), filename), dpi=100, bbox_inches='tight', pad_inches=0)\n        plt.close(fig)","metadata":{"execution":{"iopub.status.busy":"2024-02-17T09:12:26.164789Z","iopub.execute_input":"2024-02-17T09:12:26.165133Z","iopub.status.idle":"2024-02-17T09:53:03.569786Z","shell.execute_reply.started":"2024-02-17T09:12:26.165103Z","shell.execute_reply":"2024-02-17T09:53:03.568863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.makedirs(output_dir+\"/test/\", exist_ok=True)\nfor i, file_path in enumerate(tqdm(file_testlist)):\n    age = np.load(file_testlist[i])\n    distance_img = np.abs(image)                              # : EDIT HERE : select your amplitude (abs: distance, angle: velocity)\n    filter_img = srf_transform(distance_img)                  # : EDIT HERE : select your function\n\n# SAVE FIG\n    fig, axs = plt.subplots(1, 1, figsize=(width/dpi, height/dpi))\n    axs.imshow(filter_img, cmap='jet', aspect='auto')         # : EDIT HERE : select your camp\n    axs.axis('off')\n    filename = file_path.split('/')[-1].replace('.npy', '.png')\n    fig.savefig(os.path.join(output_dir, \"test\", filename), dpi=100, bbox_inches='tight', pad_inches=0)\n    plt.close(fig)","metadata":{"execution":{"iopub.status.busy":"2024-02-17T09:53:03.571149Z","iopub.execute_input":"2024-02-17T09:53:03.571451Z","iopub.status.idle":"2024-02-17T10:03:25.70761Z","shell.execute_reply.started":"2024-02-17T09:53:03.571425Z","shell.execute_reply":"2024-02-17T10:03:25.706589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torch transformers datasets evaluate pillow==9.2.0\n!pip install git+https://github.com/rwightman/pytorch-image-models.git\n","metadata":{"execution":{"iopub.status.busy":"2024-02-17T10:08:23.097266Z","iopub.execute_input":"2024-02-17T10:08:23.097661Z","iopub.status.idle":"2024-02-17T10:09:10.338244Z","shell.execute_reply.started":"2024-02-17T10:08:23.097631Z","shell.execute_reply":"2024-02-17T10:09:10.337079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.transforms as T\nfrom torch.utils.data import DataLoader, random_split\n\n# Pytorch Image model (TIMM) library: a library for state-of-the-art image classification\nimport timm\nimport timm.optim\nimport timm.scheduler\nfrom timm.data import ImageDataset, create_dataset, create_loader\nfrom timm.data.transforms_factory import create_transform\n\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom PIL import Image\n\nimport evaluate\n\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\n\nfrom tqdm.notebook import tqdm\n\nimport glob\n\nfrom sklearn.model_selection import KFold\n\nfrom copy import copy\n\nimport shutil","metadata":{"execution":{"iopub.status.busy":"2024-02-17T10:09:10.340495Z","iopub.execute_input":"2024-02-17T10:09:10.340849Z","iopub.status.idle":"2024-02-17T10:09:29.009134Z","shell.execute_reply.started":"2024-02-17T10:09:10.340819Z","shell.execute_reply":"2024-02-17T10:09:29.008183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Collect all train image path\ntrain_files = glob.glob(\"/kaggle/working/out_pre/train/**/*.png\")\ntest_files = glob.glob(\"/kaggle/working/out_pre/test/*.png\")","metadata":{"execution":{"iopub.status.busy":"2024-02-17T10:09:29.010328Z","iopub.execute_input":"2024-02-17T10:09:29.010973Z","iopub.status.idle":"2024-02-17T10:09:29.020757Z","shell.execute_reply.started":"2024-02-17T10:09:29.010939Z","shell.execute_reply":"2024-02-17T10:09:29.019427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize image\nexample = Image.open(train_files[0]).convert(\"RGB\")\ndisplay(example)","metadata":{"execution":{"iopub.status.busy":"2024-02-17T10:09:29.023212Z","iopub.execute_input":"2024-02-17T10:09:29.02383Z","iopub.status.idle":"2024-02-17T10:09:29.042545Z","shell.execute_reply.started":"2024-02-17T10:09:29.023794Z","shell.execute_reply":"2024-02-17T10:09:29.041617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transform image data based on ImageNet's mean and std\ntransforms = {\n    \"train\": T.Compose([\n        T.Resize((224, 224), interpolation=T.InterpolationMode.BICUBIC),\n        T.ToTensor(),\n        T.Normalize(mean=torch.tensor([0.4850, 0.4560, 0.4060]), std=torch.tensor([0.2290, 0.2240, 0.2250]))\n    ]),\n    \"test\": T.Compose([\n        T.Resize((224, 224), interpolation=T.InterpolationMode.BICUBIC),\n        T.ToTensor(),\n        T.Normalize(mean=torch.tensor([0.4850, 0.4560, 0.4060]), std=torch.tensor([0.2290, 0.2240, 0.2250]))\n    ])\n}","metadata":{"execution":{"iopub.status.busy":"2024-02-17T10:09:29.043615Z","iopub.execute_input":"2024-02-17T10:09:29.043918Z","iopub.status.idle":"2024-02-17T10:09:29.064844Z","shell.execute_reply.started":"2024-02-17T10:09:29.043892Z","shell.execute_reply":"2024-02-17T10:09:29.063796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"toImage = T.ToPILImage()","metadata":{"execution":{"iopub.status.busy":"2024-02-17T10:09:29.066018Z","iopub.execute_input":"2024-02-17T10:09:29.066294Z","iopub.status.idle":"2024-02-17T10:09:29.071117Z","shell.execute_reply.started":"2024-02-17T10:09:29.066266Z","shell.execute_reply":"2024-02-17T10:09:29.070048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display an example of transform images\ntoImage(transforms[\"train\"](example))","metadata":{"execution":{"iopub.status.busy":"2024-02-17T10:09:29.072437Z","iopub.execute_input":"2024-02-17T10:09:29.072827Z","iopub.status.idle":"2024-02-17T10:09:29.18073Z","shell.execute_reply.started":"2024-02-17T10:09:29.072793Z","shell.execute_reply":"2024-02-17T10:09:29.179709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assign dataset from train signal\ndataset = ImageDataset(\"/kaggle/working/out_pre/train\", transform=transforms[\"train\"])","metadata":{"execution":{"iopub.status.busy":"2024-02-17T10:09:29.181885Z","iopub.execute_input":"2024-02-17T10:09:29.182237Z","iopub.status.idle":"2024-02-17T10:09:29.20472Z","shell.execute_reply.started":"2024-02-17T10:09:29.182208Z","shell.execute_reply":"2024-02-17T10:09:29.203965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-02-17T10:09:29.205717Z","iopub.execute_input":"2024-02-17T10:09:29.206016Z","iopub.status.idle":"2024-02-17T10:09:29.261355Z","shell.execute_reply.started":"2024-02-17T10:09:29.205991Z","shell.execute_reply":"2024-02-17T10:09:29.259978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select model (List of available is shown above)\nmodel_name = \"hf_hub:timm/maxvit_base_tf_224.in1k\"    # : EDIT HERE : Change model name","metadata":{"execution":{"iopub.status.busy":"2024-02-17T10:09:29.265544Z","iopub.execute_input":"2024-02-17T10:09:29.26626Z","iopub.status.idle":"2024-02-17T10:09:29.270978Z","shell.execute_reply.started":"2024-02-17T10:09:29.266218Z","shell.execute_reply":"2024-02-17T10:09:29.270015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 25\ncriterion = nn.CrossEntropyLoss()\n\n# Cross Validation Configuration\nk_splits = 5\nmetric = evaluate.load(\"f1\")","metadata":{"execution":{"iopub.status.busy":"2024-02-17T10:09:29.272491Z","iopub.execute_input":"2024-02-17T10:09:29.272865Z","iopub.status.idle":"2024-02-17T10:09:29.776953Z","shell.execute_reply.started":"2024-02-17T10:09:29.272831Z","shell.execute_reply":"2024-02-17T10:09:29.776127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cross validation\nkf = KFold(n_splits=k_splits, shuffle=True, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-02-17T10:09:29.777985Z","iopub.execute_input":"2024-02-17T10:09:29.778279Z","iopub.status.idle":"2024-02-17T10:09:29.782987Z","shell.execute_reply.started":"2024-02-17T10:09:29.778253Z","shell.execute_reply":"2024-02-17T10:09:29.781977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Gradient Accumulation Settings\n# Set to 1 for no accumulation\ntrain_batch_size = 8\neval_batch_size = 16\nnum_accumulate = 4","metadata":{"execution":{"iopub.status.busy":"2024-02-17T10:09:29.78407Z","iopub.execute_input":"2024-02-17T10:09:29.784413Z","iopub.status.idle":"2024-02-17T10:09:29.793689Z","shell.execute_reply.started":"2024-02-17T10:09:29.784383Z","shell.execute_reply":"2024-02-17T10:09:29.792804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_eval_scores = []\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n    print(f\"Fold {fold+1} of 5\")\n\n    # Load Model\n    model = timm.create_model(model_name, pretrained=True, num_classes=7).to(device)\n\n    # Load Optimizer and Scheduler\n    optimizer = timm.optim.create_optimizer_v2(model, opt=\"AdamW\", lr=1e-3)\n    optimizer = timm.optim.Lookahead(optimizer, alpha=0.5, k=6)                           # update the slow weight every k steps\n                                                                                          # update the optimizer by combine slow weight and fast weight * alpha\n    \n    scheduler = timm.scheduler.create_scheduler_v2(optimizer, num_epochs=num_epochs)[0]\n\n    # Load Data: split train and valition set based on kfold\n    train_dataset = torch.utils.data.Subset(dataset, train_idx)\n    val_dataset = torch.utils.data.Subset(dataset, val_idx)\n    \n    train_dataloader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n    val_dataloader = DataLoader(val_dataset, batch_size=eval_batch_size, shuffle=False)\n\n    # Reset Model Info\n    info = {\n        \"metric_train\": [],\n        \"metric_val\": [],\n        \"train_loss\": [],\n        \"val_loss\": [],\n        \"best_metric_val\": -999,\n    }\n    \n    for epoch in range(num_epochs):\n        train_loss_epoch = []\n        val_loss_epoch = []\n    \n        train_preds = []\n        train_targets = []\n    \n        val_preds = []\n        val_targets = []\n    \n        num_updates = epoch * len(train_dataloader)\n    \n        ### === Train Loop === ###\n\n        model.train()\n        for idx, batch in enumerate(tqdm(train_dataloader)):\n            inputs, targets = batch\n            outputs = model(inputs.to(device))\n            loss = criterion(outputs, targets.to(device))\n    \n            loss.backward()\n    \n            # === Gradient Accumulation === #\n            if ((idx + 1) % num_accumulate == 0) or (idx + 1 == len(train_dataloader)):\n                optimizer.step()\n                scheduler.step_update(num_updates=num_updates)\n                optimizer.zero_grad()\n            # ============================= #\n    \n            train_loss_epoch.append(loss.item())\n            train_preds += outputs.argmax(-1).detach().cpu().tolist()\n            train_targets += targets.tolist()\n        ### ==================== ###\n    \n        optimizer.sync_lookahead()              # Sync slow weight and fast weight\n        scheduler.step(epoch + 1)\n    \n        ### === Evaluation Loop === ###\n        model.eval()\n        with torch.no_grad():\n            for batch in tqdm(val_dataloader):\n                inputs, targets = batch\n                outputs = model(inputs.to(device))\n                loss = criterion(outputs, targets.to(device))\n    \n                # Log Values\n                val_loss_epoch.append(loss.item())\n                val_preds += outputs.argmax(-1).detach().cpu().tolist()\n                val_targets += targets.tolist()\n        ### ======================= ###\n        \n        # Log Data\n        metric_train = metric.compute(predictions=train_preds, references=train_targets, average=\"macro\")[\"f1\"]\n        metric_val = metric.compute(predictions=val_preds, references=val_targets, average=\"macro\")[\"f1\"]\n    \n        info[\"metric_train\"].append(metric_train)\n        info[\"metric_val\"].append(metric_val)\n    \n        info[\"train_loss\"].append(np.average(train_loss_epoch))\n        info[\"val_loss\"].append(np.average(val_loss_epoch))\n    \n        if metric_val > info[\"best_metric_val\"]:\n            print(\"New Best Score!\")\n            info[\"best_metric_val\"] = metric_val\n            torch.save(model, f\"checkpoint_fold{fold}.pt\")\n        \n#         print(info)\n        print(f\"Fold: {fold} | Epoch: {epoch} | Metric: {metric_val} | Training Loss: {np.average(train_loss_epoch)} | Validation Loss: {np.average(val_loss_epoch)}\")\n    \n    # save all best metric val\n    all_eval_scores.append(info[\"best_metric_val\"])","metadata":{"execution":{"iopub.status.busy":"2024-02-17T10:09:29.795094Z","iopub.execute_input":"2024-02-17T10:09:29.795408Z","iopub.status.idle":"2024-02-17T11:40:18.090377Z","shell.execute_reply.started":"2024-02-17T10:09:29.795385Z","shell.execute_reply":"2024-02-17T11:40:18.089038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_eval_scores\nprint(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# หลังจากนี้จะเป็นโค้ดที่นำ ไปทำต่อใน Local","metadata":{}},{"cell_type":"markdown","source":"**Error Analysis**","metadata":{}},{"cell_type":"code","source":"for fold in range(k_splits):\n    predictions = []\n    references = []\n    \n    # load model\n    loaded_model = torch.load(f\"checkpoint_fold{fold}.pt\")\n    # Evaluation\n    loaded_model.eval()\n    with torch.no_grad():\n        for batch in tqdm(val_dataloader):\n            inputs, targets = batch\n            outputs = loaded_model(inputs.to(device))\n    \n            # Log Values\n            predictions += outputs.argmax(-1).detach().cpu().tolist()\n            references += targets.tolist()\n    \n    print(f\"Fold: {fold}\")\n    \n    # Confusion matrix\n    cm = confusion_matrix(references, predictions)\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n    disp.plot()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-17T13:22:26.222152Z","iopub.execute_input":"2024-02-17T13:22:26.222817Z","iopub.status.idle":"2024-02-17T13:22:46.073976Z","shell.execute_reply.started":"2024-02-17T13:22:26.222786Z","shell.execute_reply":"2024-02-17T13:22:46.072802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Technique1: Voting Classifier","metadata":{}},{"cell_type":"code","source":"answers_final = dict()\n\n# Loop for each fold\nfor fold in range(k_splits):\n    # load model\n    loaded_model = torch.load(f\"/kaggle/working/checkpoint_fold{fold}.pt\")\n\n    # Evaluation\n    model.eval() \n    with torch.no_grad():\n        for f in tqdm(test_files):\n            key = f.split(\"/\")[-1].split(\".\")[0]\n            \n            img = Image.open(f).convert(\"RGB\")\n            transformed = transforms[\"test\"](img).unsqueeze(0).to(device)\n            \n            # Collect all predicted value of each fold\n            if fold == 0:\n                answers_final[key] = [loaded_model(transformed).argmax(-1).item()]\n            else:\n                answers_final[key].append(loaded_model(transformed).argmax(-1).item())\nprint(answers_final)\nfor key in answers_final:\n    # Take most occuring number to be answer\n    answers_final[key] = stats.mode(answers_final[key])[0][0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save result of voting technique\nwith open(f\"solution_maxvit_kfold{k_splits}_voting_tech1.csv\", \"w\") as f:\n    f.write(\"id,class\\n\")\n    for name in answers_final:\n        f.write(f\"{name},{answers_final[name]}\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-02-17T13:17:35.25615Z","iopub.execute_input":"2024-02-17T13:17:35.256543Z","iopub.status.idle":"2024-02-17T13:17:35.262853Z","shell.execute_reply.started":"2024-02-17T13:17:35.256492Z","shell.execute_reply":"2024-02-17T13:17:35.261957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Technique2: Weighted Ensemble","metadata":{}},{"cell_type":"code","source":"answers_final = dict()\n\n# Loop for each fold\nfor fold in range(k_splits):\n    # load model\n    loaded_model = torch.load(f\"checkpoint_fold{fold}.pt\")\n\n    # evaluation\n    model.eval() \n    with torch.no_grad():\n        for f in tqdm(test_files):\n            key = f.split(\"/\")[-1].split(\".\")[0]\n   \n            img = Image.open(f).convert(\"RGB\")\n            transformed = transforms[\"test\"](img).unsqueeze(0).to(device)\n            \n            # Collect all predicted value of each fold \"AND\" multiply them with their evaluation scores\n            if fold == 0:\n                answers_final[key] = loaded_model(transformed).cpu().numpy() * all_eval_scores[fold]\n            else:\n                answers_final[key] = answers_final[key] + (loaded_model(transformed).cpu().numpy() * all_eval_scores[fold])\n\nanswers_raw = copy(answers_final)\n\nfor key in answers_final:\n    # Take most occuring number \n    answers_final[key] = np.argmax(answers_final[key], -1).item()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# save result of voting technique\nwith open(f\"solution_maxvit_kfold{k_splits}_weighted.csv\", \"w\") as f:\n    f.write(\"id,class\\n\")\n    for name in answers_final:\n        f.write(f\"{name},{answers_final[name]}\\n\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Technique3: Pseudo Labeling","metadata":{}},{"cell_type":"code","source":"# Prepare softmax layer\nsoftmax = nn.Softmax()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create dict for collect confidence test data\nto_move = {x: [] for x in range(7)}\nthreshold = 0.9   # How confidence of each answering\n\nfor key in answers_raw:\n    # Get the predicted class idex by considering the most probability value\n    predicted_class = np.argmax(answers_raw[key], -1).item()\n\n    # If the model's confidence in the answer more than threshold, collect it\n    if softmax(torch.tensor(answers_raw[key]))[predicted_class].item() > threshold:\n        to_move[predicted_class].append(key)\n     ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Copy confidence data to training set\nfor pred_class in to_move:\n    for item in to_move[pred_class]:\n        file_name = item.split(\"/\")[-1]\n        shutil.copyfile(item, f\"signal/signal_train/{pred_class}/{file_name}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# technique4: CSV Ensemble","metadata":{}},{"cell_type":"code","source":"all_answers = dict()\n\n# Combine all predicted value from each csv\nfor idx, submission in enumerate(all_submissions):\n    # read result csv\n    df = pd.read_csv(submission)\n    # convert csv to dictionary\n    mapper = {}\n    for i in range(len(df)):\n        mapper[df.id[i]] = df['class'][i]\n    # add each predicted value in to list\n    for key in mapper:\n        if idx == 0:\n            all_answers[key] = [mapper[key]]\n        else:\n            all_answers[key].append(mapper[key])\n     ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"real_answers = {}\n# loop for all answer\nfor key in all_answers:\n    # Take most occuring number \n    real_answers[key] = stats.mode(all_answers[key])[0]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save final result\nwith open(f\"no1-3.0.csv\", \"w\") as f:\n    f.write(\"id,class\\n\")\n    for name in real_answers:\n        f.write(f\"{name},{real_answers[name]}\\n\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"no1-3.0.csv\")\ndf['class'][0] = 6\ndf['class'][0] = 4\ndf['class'][0] = 2\ndf.to_csv(\"no1-3.0.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]}]}